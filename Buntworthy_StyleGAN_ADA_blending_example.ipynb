{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Buntworthy-StyleGAN-ADA-blending-example.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spiyer99/simpsonification/blob/master/Buntworthy_StyleGAN_ADA_blending_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeqkH_Ft5p4w"
      },
      "source": [
        "# Network blending in StyleGAN-ADA\n",
        "\n",
        "Swapping layers between two models in StyleGAN gives some interesting results. You need a base model and a second model which has been fine-tuned from the base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8KwDllv54v0"
      },
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/justinpinkney/stylegan2.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdbbJ2lI5-Sa"
      },
      "source": [
        "%cd stylegan2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0DKteHv6hJV"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install typer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8MlAfVn8bVB"
      },
      "source": [
        "Command line help for the blending function is below. Make sure that you specify either `--output-grid` to save an example image or `--output-pkl` to save the modified pkl. (Currently only the the Gs network is modified)\n",
        "\n",
        "- low_res_pkl: Path, # Pickle file from which to take low res layers\n",
        "- high_res_pkl: Path, # Pickle file from which to take high res layers\n",
        "- resolution: int, # Resolution level at which to switch between models\n",
        "- level: int  = 0, # Switch at Conv block 0 or 1?\n",
        "- blend_width: Optional[float] = None, # None = hard switch, float = smooth switch (logistic) with given width\n",
        "- output_grid: Optional[Path] = \"blended.jpg\", # Path of image file to save example grid (None = don't save)\n",
        "- seed: int = 0, # seed for random grid\n",
        "- output_pkl: Optional[Path] = None, # Output path of pickle (None = don't save)\n",
        "- verbose: bool = False, # Print out the exact blending fraction\n",
        "\n",
        "         "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fCG-AZ69ttu"
      },
      "source": [
        "Use as an example a model I fine-tuned at 256 from FFHQ to data scraped from NASA DSCOVR:EPIC satellite (https://epic.gsfc.nasa.gov/).\n",
        "\n",
        "For the output of the fine-tuned model see the tweet below (https://twitter.com/Buntworthy/status/1295445259971899393)\n",
        "\n",
        "I'm actually going to use a model from earlier in training than the result shown in the tweet, this makes the blending a bit nicer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_rGH1tAVFvm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzpUOVjBoiRe"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada.git\n",
        "%cd stylegan2-ada\n",
        "\n",
        "!nvcc test_nvcc.cu -o test_nvcc -run\n",
        "\n",
        "print('Tensorflow version: {}'.format(tf.__version__) )\n",
        "!nvidia-smi -L\n",
        "print('GPU Identified at: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3NnS2L0oqq3"
      },
      "source": [
        "!cp /content/stylegan2/blend_models.py /content/stylegan2-ada/\n",
        "!cp /content/stylegan2/training/misc.py /content/stylegan2-ada/training/\n",
        "!cp /content/stylegan2/grid_vid.py /content/stylegan2-ada/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r-q9lOkVEAn"
      },
      "source": [
        "####\n",
        "# Here you get your fine-tuned PKL and name it other.pkl\n",
        "\n",
        "!wget 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metfaces.pkl' -O other.pkl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tApMIvhgw7Gp"
      },
      "source": [
        "####\n",
        "# Here you get your FFHQ PKL and name it ffhq.pkl\n",
        "\n",
        "# !wget 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl' -O ffhq.pkl\n",
        "# ffhq_url = \"http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-ffhq-config-f.pkl\"\n",
        "# ffhq_url = \"https://hanlab.mit.edu/projects/data-efficient-gans/models/stylegan2-ffhq.pkl\"\n",
        "ffhq_url = \"https://drive.google.com/uc?id=1BUL-RIzXC7Bpnz2cn230CbA4eT7_Etp0\"\n",
        "\n",
        "FFHQ_MODEL = 'ffhq.pkl'\n",
        "# !wget $ffhq_url -O $FFHQ_MODEL\n",
        "!gdown $ffhq_url -O $FFHQ_MODEL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGrn6dyx7MaC"
      },
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "def select_items_from_list(l):\n",
        "  return [l[len(l)//4], l[len(l)//2], l[3*len(l)//4], l[-2]]\n",
        "\n",
        "def get_snapshot_number(file_path):\n",
        "  return int(str(Path(file_path).stem).split('-')[-1])\n",
        "\n",
        "RESULTS_ITERATION = 8\n",
        "SNAPSHOTS_PATH = f'/content/drive/MyDrive/stylegan2/results{RESULTS_ITERATION}/*/network-snapshot-*.pkl'\n",
        "SNAPSHOTS = glob.glob(SNAPSHOTS_PATH)\n",
        "SNAPSHOTS = sorted(SNAPSHOTS, key = lambda x: get_snapshot_number(x))\n",
        "# SNAPSHOTS = select_items_from_list(SNAPSHOTS)\n",
        "\n",
        "# !gdown https://drive.google.com/uc?id=1_QysUKfed1-_x9e5off2WWJKp1yUcidu\n",
        "# SNAPSHOTS = ['ukiyoe-256-slim-diffAug-002789.pkl']\n",
        "\n",
        "SNAPSHOTS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVSbNt1F9K5O"
      },
      "source": [
        "from IPython.display import Image "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HujlFGIWVnFX"
      },
      "source": [
        "!mkdir -p \"/content/drive/MyDrive/colab_data/blend\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EpOyZSB-yMF"
      },
      "source": [
        "Time to try out a bunch of different resolutions for the switch and display the results. Remember the earth model provides the low-resolution layers, i.e. the \"structure\", and the original faces model the high-resolution, i.e. the \"texture\"\n",
        "\n",
        "I'm going to run the main function in blend_models.py in a python loop, but you can also run it from the command line, something like\n",
        "\n",
        "`python blend_models.py epic-slim-256-000040.pkl stylegan2-ffhq.pkl 64 --output-grid \"blended.jpg\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vve_EHFl8EbQ"
      },
      "source": [
        "import blend_models\n",
        "\n",
        "resolutions = (64, 128, 256)\n",
        "#resolutions = (16, 32, 64)\n",
        "for model in SNAPSHOTS[5:]:\n",
        "  for res in resolutions:\n",
        "    filename = f\"/content/drive/My Drive/colab_data/blend/other-ffhq-blended-{Path(model).stem}-{res}.jpg\"\n",
        "    blend_models.main(model, \"ffhq.pkl\", res, output_grid=filename)\n",
        "    img = Image(filename=filename)\n",
        "    print(f\"blending {Path(model).stem} at {res}x{res}\")\n",
        "    display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9MCjTAuc5IZ"
      },
      "source": [
        "resolutions = (64, 128, 256)\n",
        "for model in SNAPSHOTS:\n",
        "  for res in resolutions:\n",
        "    filename = f\"/content/drive/My Drive/colab_data/blend/ffhq-other-blended-{Path(model).stem}-{res}.jpg\"\n",
        "    blend_models.main(\"ffhq.pkl\", model, res, output_grid=filename)\n",
        "    img = Image(filename=filename)\n",
        "    print(f\"blending {Path(model).stem} at {res}x{res}\")\n",
        "    display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-sTQ92QDd71"
      },
      "source": [
        "\n",
        "%cd /content/stylegan2-ada/\n",
        "#Here you choose the layer for the swapping in which your fine-tuned PKL is on the \"Low res\" side\n",
        "!python blend_models.py $SNAPSHOTS[-1] ffhq.pkl 64 --output-pkl=\"other-ffhq-blended-64.pkl\"\n",
        "#Here you choose the layer for the swapping in which the FFHQ PKL is on the \"Low res\" side\n",
        "!python blend_models.py ffhq.pkl $SNAPSHOTS[-1] 8 --output-pkl=\"ffhq-other-blended-8.pkl\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHAiguCfsLK_"
      },
      "source": [
        "#Save a copy of your blended PKLs in your drive\n",
        "!cp \"other-ffhq-blended-64.pkl\" \"/content/drive/My Drive/colab_data/blend/MetFaces-FFHQ-blended-64.pkl\"\n",
        "!cp \"ffhq-other-blended-8.pkl\"  \"/content/drive/My Drive/colab_data/blend/FFHQ-MetFaces-blended-8.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng9pNdOmyCrB"
      },
      "source": [
        "#Make interpolation animations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbrTL1XYZvh9"
      },
      "source": [
        "%cd /content/stylegan2-ada/\n",
        "import blend_models\n",
        "!python grid_vid.py ffhq-other-blended-8.pkl\n",
        "!mv \"output.mp4\" \"/content/drive/My Drive/colab_data/blend/FFHQ-MetFaces-blended-8.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MworwYq9v0f4"
      },
      "source": [
        "#%cd /content/stylegan2/\n",
        "%cd /content/stylegan2-ada/\n",
        "import blend_models\n",
        "!python grid_vid.py \"other-ffhq-blended-64.pkl\"\n",
        "!mv \"output.mp4\" \"/content/drive/My Drive/colab_data/blend/MetFaces-FFHQ-blended-64.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mRpqQA6oUFq"
      },
      "source": [
        "#%cd /content/stylegan2/\n",
        "%cd /content/stylegan2-ada/\n",
        "import blend_models\n",
        "!python grid_vid.py $SNAPSHOTS[-1]\n",
        "!mv \"output.mp4\" \"/content/drive/My Drive/colab_data/blend/MetFaces.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghcnUld8mmWn"
      },
      "source": [
        "#%cd /content/stylegan2/\n",
        "%cd /content/stylegan2-ada/\n",
        "import blend_models\n",
        "!python grid_vid.py ffhq.pkl\n",
        "!mv \"output.mp4\" \"/content/drive/My Drive/colab_data/blend/FFHQ.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ216n-OxuhX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}